name: Launch job

on:
  workflow_call:
    inputs:
      runs_on:
        required: false
        type: string
        default: docker

      job_path:
        required: false
        type: string
        default: None
      
      job_name:
        required: false
        type: string
        default: None

      singularity_image:
          required: false
          type: string
          default: None

      enable_tunnel:
        required: false
        type: boolean
        default: false

      jupyter_port:
        required: false
        type: string
        default: "8888"

      extra_port:
        required: false
        type: string
        default: "8080"

jobs:
  launch-job:
    timeout-minutes: 43200
    runs-on: ${{ inputs.runs_on }}
    env:
      REMOTE_HOST: ${{ secrets.REMOTE_HOST }}
      REMOTE_USER: ${{ secrets.REMOTE_USER }}
      REMOTE_GROUP: ${{ secrets.REMOTE_GROUP }}
      GPFS_SINGULARITY_IMAGE_REGISTRY_PATH: ${{ vars.GPFS_SINGULARITY_IMAGE_REGISTRY_PATH }}
      GPFS_JUPYTER_WORKING_DIR: ${{ vars.GPFS_JUPYTER_WORKING_DIR }}
      GPFS_MODELS_REGISTRY_PATH: ${{ vars.GPFS_MODELS_REGISTRY_PATH }}
      MLFLOW_TRACKING_SERVER_URL: ${{vars.MLFLOW_TRACKING_SERVER_URL}}
      
    steps:
      - uses: actions/checkout@v4
   
      - name: Set ENV variables
        run: |
          JOB_REPO_NAME=${GITHUB_REPOSITORY#$GITHUB_REPOSITORY_OWNER/}
          JOB_BRANCH=${GITHUB_HEAD_REF:-${GITHUB_REF#refs/heads/}}

          echo "JOB_REPO_NAME=${JOB_REPO_NAME}" >> $GITHUB_ENV
          echo "JOB_BRANCH=${JOB_BRANCH}" >> $GITHUB_ENV
          
          
          if [ -z "${{ inputs.singularity_image }} ] || [ "${{ inputs.singularity_image }}" == "None" ]; then
            echo "JOB_SINGULARITY_IMAGE=$(echo ${JOB_REPO_NAME}/${JOB_BRANCH}.sif | awk '{print tolower($0)}')" >> $GITHUB_ENV
          else
            echo "JOB_SINGULARITY_IMAGE=${{ inputs.singularity_image }}" >> $GITHUB_ENV
          fi
          
          if [ -z "${{ inputs.job_name }}" ] || [ "${{ inputs.job_name }}" == "None" ]; then 
            echo "JOB_NAME=${JOB_REPO_NAME}-${{ github.run_number }}" >> $GITHUB_ENV
          else
            echo "JOB_NAME=${{ inputs.job_name }}" >> $GITHUB_ENV
          fi

          if [ -z "${{ inputs.job_path }}" ] || [ "${{ inputs.job_path }}" == "None" ]; then
            echo "JOB_PATH=/gpfs/scratch/${{ secrets.REMOTE_GROUP }}/${{ secrets.REMOTE_USER }}/jobs/${JOB_REPO_NAME}/${{ github.run_number }}" >> $GITHUB_ENV
          else
            echo "JOB_PATH=${{ inputs.job_path }}" >> $GITHUB_ENV
          fi
      
      - name: Job name and diractory where the job will be launched
        run: |
          echo "Directory: ${{ env.JOB_PATH }}"
          echo "Job name: ${{ env.JOB_NAME }}"
      
      - name: Set Socket variables
        if: ${{ inputs.enable_tunnel }} == 'true'
        run: |
          export EXTRA_SOCKET=${JOB_PATH}/socket${{ inputs.extra_port }}.sock
          export JUPYTER_SOCKET=${JOB_PATH}/socket${{ inputs.jupyter_port }}.sock
          echo "EXTRA_SOCKET=${EXTRA_SOCKET}" >> $GITHUB_ENV
          echo "JUPYTER_SOCKET=${JUPYTER_SOCKET}" >> $GITHUB_ENV

          echo "JOB_SSH_TUNNEL_COMMAND=ssh -N -L ${{ inputs.extra_port }}:${EXTRA_SOCKET} -L ${{ inputs.jupyter_port }}:${JUPYTER_SOCKET} ${{ secrets.REMOTE_USER }}@${{ secrets.REMOTE_HOST }}" >> $GITHUB_ENV

      - name: Save job related env vars to file
        run: |
          env | grep -e '^JOB_' -e '^GPFS_' > src/job.env
      
      - name: Replace SBATCH Directives with env variables
        run: |
          sed -i "s|\%JOB_NAME%|${JOB_NAME}|" src/launch.sh
          sed -i "s|\%JOB_PATH%|${JOB_PATH}|" src/launch.sh
                    
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          architecture: 'x64'
      - uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Display Python version
        run: python -c "import sys; print(sys.version)"

      - name: Install hpc rocket from source
        run: pip install hpc-rocket

      - name: Launch job
        run: |
          output=$(hpc-rocket launch rocket.yml)
          echo "SLURM_JOB_ID=$(echo "$output" | grep -oE 'job [0-9]+' | grep -oE '[0-9]+')" >> $GITHUB_ENV
      
      - name: Setup mlflow
        id: mlflow
        uses: langtech-bsc/action_rsync_mlflow@main
        with:
          experiment_name: ${GITHUB_REPOSITORY#$GITHUB_REPOSITORY_OWNER/}
          remote_host: ${{ secrets.REMOTE_HOST }}
          remote_user: ${{ secrets.REMOTE_USER }}
          remote_source_path: ${{ env.JOB_PATH }}/logs
          traking_url: ${{ env.MLFLOW_TRACKING_SERVER_URL }}
          run_name: ${{ env.SLURM_JOB_ID }} 
          schedule: 'true'
      
      - name: Check Mlflow traking url
        run: |
          if [ -z "${{ steps.mlflow.outputs.artifact_url }}" ]; then
            echo "Mlflow API is not available, please contact MLOps team or try running the workflow again"
            echo "If you running the workflow locally with act, ensure you have BSC VPN conneted"
            exit 1
          else
            echo "${{ steps.mlflow.outputs.artifact_url }}"
          fi

      - name: Watch job
        id: watch
        run: | 
         #Live status
         echo "Check the logs at: ${{ steps.mlflow.outputs.artifact_url }}"
         echo "Keep in mind that synchronization may take a little bit"
         nohup $(while true; do
            # hpc-rocket status rocket.yml --jobid ${{ env.SLURM_JOB_ID }} 
            running=$(hpc-rocket status rocket.yml --jobid ${{ env.SLURM_JOB_ID }}  | grep "RUNNING" | wc -l)
            if [[ $running > 0 ]]
            then
              nohup ml_flow -t sync > ${{ steps.mlflow.outputs.sync_dir }}/mlflow.log 2>&1 < /dev/null &
              echo "mlflow_pid=$!" >> $GITHUB_OUTPUT
              if [ '${{ inputs.enable_tunnel }}' = 'true' ]; then
                SLURM_JOB_NODE=$(ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ${{ secrets.REMOTE_USER }}@${{ secrets.REMOTE_HOST }} "squeue -j ${{ env.SLURM_JOB_ID }}  -o '%R'" | tail -1)
                
                if [[ $SLURM_JOB_NODE == *"["* ]]; then    
                    IFS='[' read -r prefix range <<< "$SLURM_JOB_NODE"  # Split by '['
                    if [[ $range == *"-"* ]]; then                      # Handle range (contains '-')
                        IFS='-' read -r start end <<< "$range"
                    elif [[ $range == *","* ]]; then                    # Handle list (contains ',')
                        IFS=',' read -r start rest <<< "$range"
                    else
                        start=$range
                    fi
                    
                    start=${start%]*}                                   # Remove ']' from the start variable
                    result="${prefix}${start}"                          # Join prefix with start
                    SLURM_JOB_NODE=$result
                fi

                ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ${{ secrets.REMOTE_USER }}@${{ secrets.REMOTE_HOST }} "rm -rf ${{ env.EXTRA_SOCKET }} ${{ env.JUPYTER_SOCKET }}; ssh -Nf -L ${{ env.EXTRA_SOCKET }}:localhost:${{ inputs.extra_port }} -L ${{ env.JUPYTER_SOCKET }}:localhost:${{ inputs.jupyter_port }} ${SLURM_JOB_NODE} && chmod 660 -f ${{ env.EXTRA_SOCKET }} ${{ env.JUPYTER_SOCKET }} 123abc "
                echo "${{ env.JOB_SSH_TUNNEL_COMMAND }}" > ${{ steps.mlflow.outputs.sync_dir }}/tunnel.log
              fi
              break
            fi
            sleep 5
          done)  > ./tmp 2>&1 < /dev/null &

          (while true; do
            hpc-rocket status rocket.yml --jobid ${{ env.SLURM_JOB_ID }} 
            sleep 20
          done) &
         hpc-rocket watch rocket.yml --jobid ${{ env.SLURM_JOB_ID }} 
        shell: bash
        continue-on-error: true

      - name: Kill mlflow
        if: always()
        run: | 
          kill -9 ${{ steps.watch.outputs.mlflow_pid }}
        continue-on-error: true

      - name: Check on success
        if: steps.watch.outcome == 'success'
        run: | 
          ml_flow -t stop
          exit 0

      - name: Check on failures
        if: steps.watch.outcome != 'success'
        run: | 
          ml_flow -t stop --failed
          exit 1
