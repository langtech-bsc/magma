name: Launch job

on:
  workflow_call:
    inputs:
      runs_on:
        required: false
        type: string
        default: magma-runner-set

      job_path:
        required: false
        type: string
        default: ''
      
      job_name:
        required: false
        type: string
        default: ''

      slurm_sbatch_params: # It recives a string of sbatch parameters to add launch file. E.g, #SBATCH time=00-1:00:00$$#SBATCH gres=gpu:2$$#SBATCH ntasks=1$$#SBATCH cpus-per-task= 20"
        required: false
        type: string
        default: ''
        
      slurm_command: # A bash or sh command to add inside the slurm job file
        required: false
        type: string
        default: ''

      singularity:
        required: false
        type: string
        default: 'python-jupyter.sif'

      old_image:
        required: false
        type: string
        default: 'null'

      enable_tunnel:
        required: false
        type: boolean
        default: false

      enable_jupyter:
        required: false
        type: boolean
        default: false

      jupyter_singularity:
        required: false
        type: string
        default: 'python-jupyter.sif'

      enable_tgi:
        required: false
        type: boolean
        default: false

      tgi_singularity:
        required: false
        type: string
        default: 'text-generation-inference.2.1.1.sif'

      tgi_model:
        required: false
        type: string
        default: 'Mistral-7B-Instruct-v0.3'

      tgi_params:
        required: false
        type: string
        default: ''

      enable_vllm:
        required: false
        type: boolean
        default: false

      vllm_singularity:
        required: false
        type: string
        default: 'vllm-inference.sif'

      vllm_model:
        required: false
        type: string
        default: 'Mistral-7B-Instruct-v0.3'

      vllm_params:
        required: false
        type: string
        default: ''

      enable_finetune:
        required: false
        type: boolean
        default: false

      finetune_params:
        required: false
        type: string
        default: ''

      finetune_output_model_name:
        required: false
        type: string
        default: ''

      remote_job:
        required: false
        type: string
        default: 'NONE'

      remote_job_sandbox:
        required: false
        type: boolean
        default: false

      remote_job_ldconfig:
        required: false
        type: boolean
        default: false

jobs:
  launch-job:
    timeout-minutes: 43200
    runs-on: ${{ inputs.runs_on }}
    env:
      PORT1: "8888"
      PORT2: "8080"
      EMAIL: ""
      JOB_NAME: ${{ inputs.job_name }}
      JOB_TUNNELS_PATH: /gpfs/scratch/${{ secrets.REMOTE_GROUP }}/${{ secrets.REMOTE_USER }}/tunnels
      JOB_SINGULARITY_IMAGE: ${{ vars.GPFS_SINGULARITY_IMAGE_REGISTRY_PATH }}/${{ inputs.singularity }}
      JOB_ENABLE_TUNNEL: ${{ inputs.enable_tunnel }}
      JOB_ENABLE_JUPYTER: ${{ inputs.enable_jupyter }}
      GPFS_JUPYTER_SINGULARITY: ${{ vars.GPFS_SINGULARITY_IMAGE_REGISTRY_PATH }}/${{ inputs.jupyter_singularity }}
      JOB_TGI_ENABLE: ${{ inputs.enable_tgi }}
      GPFS_TGI_SINGULARITY: ${{ vars.GPFS_SINGULARITY_IMAGE_REGISTRY_PATH }}/${{ inputs.tgi_singularity }}
      GPFS_TGI_MODEL: ${{ inputs.tgi_model }}

      JOB_VLLM_ENABLE: ${{ inputs.enable_vllm }}
      GPFS_VLLM_SINGULARITY: ${{ vars.GPFS_SINGULARITY_IMAGE_REGISTRY_PATH }}/${{ inputs.vllm_singularity }}
      GPFS_VLLM_MODEL: ${{ inputs.vllm_model }}

      JOB_FINETUNE_ENABLE: ${{ inputs.enable_finetune }}
      JOB_FINETUNE_OUTPUT_MODEL_NAME: ${{ inputs.finetune_output_model_name }}

    steps:
      - uses: actions/checkout@v4

      - uses: langtech-bsc/magma/actions/set_secrets_to_env_action@main
        with:
          must_exist: 'REMOTE_HOST,REMOTE_USER,REMOTE_GROUP,SSH_PRIVATE_KEY'
          exclude: 'SSH_PRIVATE_KEY'
          json: ${{ toJson(secrets) }}
          name: 'secret'

      - uses: langtech-bsc/magma/actions/set_secrets_to_env_action@main
        with:
          must_exist: 'MLFLOW_TRACKING_SERVER_URL,GPFS_SINGULARITY_IMAGE_REGISTRY_PATH,GPFS_MODELS_REGISTRY_PATH'
          json: ${{ toJson(vars) }}
          name: 'secret'

      - uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}
  
      - name: Set extra variables to env
        run: |
          JOB_REPO_NAME=${GITHUB_REPOSITORY#$GITHUB_REPOSITORY_OWNER/}
          JOB_BRANCH=${GITHUB_HEAD_REF:-${GITHUB_REF#refs/heads/}}

          echo "JOB_REPO_NAME=${JOB_REPO_NAME}" >> "$GITHUB_ENV"
          echo "JOB_BRANCH=${JOB_BRANCH}" >> "$GITHUB_ENV"
          
          if [ '${{ env.JOB_ENABLE_JUPYTER }}' = 'true' ] || [ '${{ env.JOB_TGI_ENABLE }}' = 'true' ] || [ '${{ env.JOB_VLLM_ENABLE }}' = 'true' ]; then
            echo "JOB_ENABLE_TUNNEL=true" >> "$GITHUB_ENV"
            echo "JOB_SLEEP=true" >> "$GITHUB_ENV"
          fi

          if [ -z "${{ env.JOB_NAME }}" ]; then 
            echo "JOB_NAME=${JOB_REPO_NAME}-${{ github.run_number }}" >> "$GITHUB_ENV"
          fi

          if [ -n "${{ inputs.tgi_params }}" ]; then
            echo 'JOB_TGI_PARAMS="${{ inputs.tgi_params }}"' >> "$GITHUB_ENV"
          fi

          if [ -n "${{ inputs.vllm_params }}" ]; then
            echo 'JOB_VLLM_PARAMS="${{ inputs.vllm_params }}"' >> "$GITHUB_ENV"
          fi

          if [ -z "${{ inputs.job_path }}" ]; then
            JOB_PATH_TMP="/gpfs/scratch/${{ secrets.REMOTE_GROUP }}/${{ secrets.REMOTE_USER }}/jobs/${JOB_REPO_NAME}/${{ github.run_number }}"
          else
            JOB_PATH_TMP="/gpfs/scratch/${{ secrets.REMOTE_GROUP }}/${{ secrets.REMOTE_USER }}/jobs/${{inputs.job_path}}"
          fi

          echo "JOB_PATH=$JOB_PATH_TMP" >> "$GITHUB_ENV"
          echo "JOB_LOGS_PATH=$JOB_PATH_TMP/logs" >> "$GITHUB_ENV"
      
      - name: Log job variables (Job path, job name, etc)
        run: |
          echo "Job variables"
          echo "JOB_PATH: ${{env.JOB_PATH}}"
          echo "JOB_NAME: ${{env.JOB_NAME}}"
          echo "JOB_LOGS_PATH: ${{env.JOB_LOGS_PATH}}"
          echo "GPFS_SINGULARITY: ${{env.GPFS_SINGULARITY}}"
          echo "JOB_ENABLE_TUNNEL: ${{env.JOB_ENABLE_TUNNEL}}"
          echo "JOB_ENABLE_JUPYTER: ${{env.JOB_ENABLE_JUPYTER}}"
          echo "GPFS_JUPYTER_SINGULARITY: ${{env.GPFS_JUPYTER_SINGULARITY}}"
          echo ""
          echo "JOB_TGI_ENABLE: ${{env.JOB_TGI_ENABLE}}"
          echo "GPFS_TGI_SINGULARITY: ${{env.GPFS_TGI_SINGULARITY}}"
          echo "GPFS_TGI_MODEL: ${{env.GPFS_TGI_MODEL}}"
          echo "JOB_TGI_PARAMS: ${{env.JOB_TGI_PARAMS}}"
          echo ""
          echo "JOB_VLLM_ENABLE: ${{env.JOB_VLLM_ENABLE}}"
          echo "GPFS_VLLM_SINGULARITY: ${{env.GPFS_VLLM_SINGULARITY}}"
          echo "GPFS_VLLM_MODEL: ${{env.GPFS_VLLM_MODEL}}"
          echo "GPFS_VLLM_PARAMS: ${{env.GPFS_VLLM_PARAMS}}"
          echo ""
          
          echo "JOB_FINETUNE_ENABLE: ${{env.JOB_FINETUNE_ENABLE}}"
          echo "JOB_FINETUNE_OUTPUT_MODEL_NAME: ${{env.JOB_FINETUNE_OUTPUT_MODEL_NAME}}"
          echo ""
          echo "JOB_SLEEP: ${{env.JOB_SLEEP}}"
          echo "REMOTE_JOB: ${{inputs.remote_job}}"

          echo ""
          echo "SLURM_PARAMS: ${{inputs.slurm_sbatch_params}}"
          echo "SLURM_COMMAND: ${{inputs.slurm_command}}"

      - name: Set Socket variables
        if: ${{ env.JOB_ENABLE_TUNNEL == 'true' || env.JOB_ENABLE_TUNNEL == true}}
        run: |
          EXTRA_SOCKET=socket${{ env.PORT2 }}.sock
          JUPYTER_SOCKET=socket${{ env.PORT1 }}.sock
          echo "EXTRA_SOCKET=${EXTRA_SOCKET}" >> "$GITHUB_ENV"
          echo "JUPYTER_SOCKET=${JUPYTER_SOCKET}" >> "$GITHUB_ENV"

      - uses: langtech-bsc/magma/actions/remote_job@main
        if: ${{ contains(fromJson('["install", "build", "ldconfig"]'), inputs.remote_job) }}
        with:
          job: ${{inputs.remote_job}}
          remote_singularity_registry_path: ${{env.GPFS_SINGULARITY_IMAGE_REGISTRY_PATH}}
          image: ${{ inputs.singularity }}
          sandbox: ${{inputs.remote_job_sandbox}}
          ldconfig: ${{inputs.remote_job_ldconfig}}
          remote_path: ${{env.JOB_PATH}}
          remote_user: ${{secrets.REMOTE_USER}}
          remote_host: ${{secrets.REMOTE_HOST}}
          old_image: ${{inputs.old_image}}
          local_dir: 'src'

      - name: Save job related env vars to file
        run: |
          mkdir -p src/logs
          env | grep -e '^JOB_' -e '^GPFS_' -e '^REMOTE_GROUP' > src/job.env
          HASHED_NAME=$(echo -n "${{env.JOB_NAME}}" | md5sum | awk '{print $1}')
          echo "HASHED_NAME=$HASHED_NAME" >> "$GITHUB_ENV"
          echo "JOB_SSH_TUNNEL_COMMAND=echo 'CONNECTED!'; ssh -N -L ${{ env.PORT2 }}:${{ env.JOB_TUNNELS_PATH }}/${HASHED_NAME}-${{env.EXTRA_SOCKET}} -L ${{ env.PORT1 }}:${{ env.JOB_TUNNELS_PATH }}/${HASHED_NAME}-${{env.JUPYTER_SOCKET}} ${{ secrets.REMOTE_USER }}@${{ secrets.REMOTE_HOST }}" >> $GITHUB_ENV
    
      - name: Add slurm variables
        if: ${{ ! contains(fromJson('["install", "build", "ldconfig"]'), inputs.remote_job) &&  inputs.slurm_sbatch_params != '' }}
        run: |
          
          if [ -f src/launch.sh ]; then
            echo "The launch.sh can't be inside the src directory. Please remove it or change the name."
            exit 1
          fi
          
          mkdir -p src
          file="src/launch.sh"
          echo "#!/bin/sh" > $file
          echo "${{inputs.slurm_sbatch_params}}" | tr '#' '\n' | awk '{print "#SBATCH " $0}' >> $file
          echo '#SBATCH --job-name=${{ env.JOB_NAME }}' >> $file
          echo '#SBATCH --output=${{ env.JOB_LOGS_PATH  }}/output.log' >> $file
          echo '#SBATCH --error=${{ env.JOB_LOGS_PATH  }}/error.log' >>  $file
  
      - uses: langtech-bsc/magma/actions/addons@main
        if: ${{ contains(fromJson('["install", "build", "ldconfig"]'), inputs.remote_job) }}
        with:
          file: "src/launch.sh"
          job_path: ${{env.JOB_PATH}}

      - uses: langtech-bsc/magma/actions/addons@main
        if: ${{ ! contains(fromJson('["install", "build", "ldconfig"]'), inputs.remote_job) }}
        with:
          file: "src/launch.sh"
          jupyter: ${{ env.JOB_ENABLE_JUPYTER }}
          tgi: ${{ env.JOB_TGI_ENABLE }}
          vllm: ${{ env.JOB_VLLM_ENABLE }}
          finetune: ${{ env.JOB_FINETUNE_ENABLE }}
          finetune_params: ${{ inputs.finetune_params }}
          sleep: ${{ env.JOB_SLEEP }}
          email: ${{env.EMAIL}},${{secrets.USER_EMAIL}}
          job_path: ${{env.JOB_PATH}}
          slurm_command: ${{inputs.slurm_command}}

      - name: Pre launch modifications
        run: |
          sed -i "s|\%JOB_NAME%|${{ env.JOB_NAME }}|" src/launch.sh
          sed -i "s|\%JOB_LOGS_PATH%|${{ env.JOB_LOGS_PATH  }}|" src/launch.sh
          sed -i "s|\%JOB_PATH%|${{ env.JOB_PATH  }}|" src/launch.sh

      - name: Launch job
        run: |
          #Launching job...
          tempfile=$(mktemp)
          ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ${{ secrets.REMOTE_USER }}@${{ secrets.REMOTE_HOST }} "mkdir -p ${{ env.JOB_LOGS_PATH }} && rm -fr ${{ env.JOB_LOGS_PATH }}/*"
          hpc-rocket launch rocket.yml 2>&1 | tee "$tempfile"
          output=$(cat "$tempfile")
          echo "SLURM_JOB_ID=$(echo "$output" | grep -oE 'job [0-9]+' | grep -oE '[0-9]+')" >> "$GITHUB_ENV"
          ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ${{ secrets.REMOTE_USER }}@${{ secrets.REMOTE_HOST }} "chmod -R g+r ${{ env.JOB_PATH }}"
      
      # This is requiered to use nested run option in mlflow
      # - name: Install mlflow
      #   run: pip install mlflow==2.15.1
      
      - name: Setup mlflow
        id: mlflow
        uses: langtech-bsc/magma/actions/mlflow@main
        with:
          experiment_name: ${GITHUB_REPOSITORY#$GITHUB_REPOSITORY_OWNER/}
          remote_host: ${{ secrets.REMOTE_HOST }}
          remote_user: ${{ secrets.REMOTE_USER }}
          remote_source_path: ${{ env.JOB_LOGS_PATH }}
          traking_url: ${{ env.MLFLOW_TRACKING_SERVER_URL }}
          run_name: ${{ env.SLURM_JOB_ID }}_${{env.JOB_BRANCH}}
          schedule: 'true'
        continue-on-error: true
      
      - name: Monitoring and status (CHECK THE LOGS HERE)
        run: |
          if [ -z "${{ steps.mlflow.outputs.artifact_url }}" ]; then
            echo "Mlflow API is not available, please contact MLOps team or try running the workflow again"
            echo "If you are running the workflow locally with act, ensure you have BSC VPN connected"
            hpc-rocket cancel rocket.yml --jobid ${{ env.SLURM_JOB_ID }}
            echo "Job canceled: ${{ env.SLURM_JOB_ID }}"
            exit 1
          else
            echo "Check the logs at: ${{ steps.mlflow.outputs.artifact_url }}"
            echo "Keep in mind that synchronization may take a little bit"
          fi

      - name: Watch job
        id: watch
        run: | 
          #Live status
          echo "Check the logs at: ${{ steps.mlflow.outputs.artifact_url }}"
          echo "Keep in mind that synchronization may take a little bit"

          hpc-rocket status rocket.yml --jobid ${{ env.SLURM_JOB_ID }}
          nohup bash -c '
            run_job_monitor() {
              while true; do
                running=$(hpc-rocket status rocket.yml --jobid ${{ env.SLURM_JOB_ID }} | grep "RUNNING" | wc -l)

                if [ "$running" -gt 0 ]; then
                  nohup ml_flow -t sync > ${{ steps.mlflow.outputs.sync_dir }}/mlflow.log 2>&1 &
                  echo "mlflow_pid=$!" >> $GITHUB_OUTPUT

                  if [ '${{ env.JOB_ENABLE_TUNNEL }}' = 'true' ]; then
                    SLURM_JOB_NODE=$(ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
                                        ${{ secrets.REMOTE_USER }}@${{ secrets.REMOTE_HOST }} \
                                        "scontrol show hostname \$(squeue -j ${{ env.SLURM_JOB_ID }} -o '%R' --noheader) | head -n 1")
                    echo $SLURM_JOB_NODE > ${{ steps.mlflow.outputs.sync_dir }}/debug.log

                    CHANGE_DIR="cd ${{ env.JOB_PATH }}"
                    REMOVE_SOCKET="rm -rf ${{ env.EXTRA_SOCKET }} ${{ env.JUPYTER_SOCKET }}"
                    CREATE_SOCKET="ssh -Nf -L ./${{ env.EXTRA_SOCKET }}:localhost:${{ env.PORT2 }} -L ./${{ env.JUPYTER_SOCKET }}:localhost:${{ env.PORT1 }} ${SLURM_JOB_NODE}"
                    PERMISSION_SOCKET="cd ${{ env.JOB_PATH }} && chmod 660 ${{ env.EXTRA_SOCKET }} ${{ env.JUPYTER_SOCKET }}"
                    CREATE_LINKS="mkdir -p ${{ env.JOB_TUNNELS_PATH }} && cd ${{ env.JOB_TUNNELS_PATH }} && ln -sf ${{ env.JOB_PATH }}/${{ env.EXTRA_SOCKET }} ${{env.HASHED_NAME}}-${{env.EXTRA_SOCKET}} && ln -sf ${{ env.JOB_PATH }}/${{ env.JUPYTER_SOCKET }} ${{env.HASHED_NAME}}-${{env.JUPYTER_SOCKET}}"

                    echo "$CHANGE_DIR && $REMOVE_SOCKET && $CREATE_SOCKET && $PERMISSION_SOCKET" >> ${{ steps.mlflow.outputs.sync_dir }}/debug.log
                    echo "$CREATE_LINKS" >> ${{ steps.mlflow.outputs.sync_dir }}/debug.log
                    sleep 20

                    ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ${{ secrets.REMOTE_USER }}@${{ secrets.REMOTE_HOST }} "$CHANGE_DIR && $REMOVE_SOCKET && $CREATE_SOCKET && $PERMISSION_SOCKET" 

                    ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ${{ secrets.REMOTE_USER }}@${{ secrets.REMOTE_HOST }} "$CREATE_LINKS" 
                    echo "${{ env.JOB_SSH_TUNNEL_COMMAND }}" > ${{ steps.mlflow.outputs.sync_dir }}/tunnel.log
                 
                  fi

                  echo ""
                  hpc-rocket status rocket.yml --jobid ${{ env.SLURM_JOB_ID }}
                  echo ""
                  
                  echo "   ✨___           ___           ___           ___                       ___           ___         "
                  echo "    /\  \         /\__\         /\__\         /\__\          ___        /\__\         /\  \        "
                  echo "   /::\  \       /:/  /        /::|  |       /::|  |        /\  \      /::|  |       /::\  \       "
                  echo "  /:/\:\  \     /:/  /        /:|:|  |      /:|:|  |        \:\  \    /:|:|  |      /:/\:\  \      "
                  echo " /::\~\:\  \   /:/  /  ___   /:/|:|  |__   /:/|:|  |__      /::\__\  /:/|:|  |__   /:/  \:\  \     "
                  echo "/:/\:\ \:\__\ /:/__/  /\__\ /:/ |:| /\__\ /:/ |:| /\__\  __/:/\/__/ /:/ |:| /\__\ /:/__/_\:\__\    "
                  echo "\/_|::\/:/  / \:\  \ /:/  / \/__|:|/:/  / \/__|:|/:/  / /\/:/  /    \/__|:|/:/  / \:\  /\ \/__/    "
                  echo "   |:|::/  /   \:\  /:/  /      |:/:/  /      |:/:/  /  \::/__/         |:/:/  /   \:\ \:\__\      "
                  echo "   |:|\/__/     \:\/:/  /       |::/  /       |::/  /    \:\__\         |::/  /     \:\/:/  /   🌟 "
                  echo "   |:|  |        \::/  /        /:/  /        /:/  /      \/__/         /:/  /       \::/  /       "
                  echo "    \|__|         \/__/         \/__/         \/__/                     \/__/         \/__/  ✨    "
                   
                  # create here figlets: https://www.askapache.com/online-tools/figlet-ascii/
                  break
                fi
                sleep 10
              done
            }

          run_job_monitor
          ' 2>> ${{ steps.mlflow.outputs.sync_dir }}/debug.log &

          hpc-rocket watch rocket.yml --jobid ${{ env.SLURM_JOB_ID }} 
        shell: bash
        continue-on-error: true

      - name: Kill mlflow
        if: always()
        run: | 
          kill -9 ${{ steps.watch.outputs.mlflow_pid }}
        continue-on-error: true

      - name: Check on success
        if: steps.watch.outcome == 'success'
        run: | 
          ml_flow -t stop
          exit 0

      - name: Check on failures
        if: steps.watch.outcome != 'success'
        run: | 
          ml_flow -t stop --failed
          exit 1
